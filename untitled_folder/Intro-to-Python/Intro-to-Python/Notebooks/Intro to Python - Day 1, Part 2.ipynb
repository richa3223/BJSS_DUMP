{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\">\n",
    "    <div class=\"columm\">\n",
    "        <h1 style=\"position: absolute;font-size: 300%;\">Introduction to Python: Day 1</h1>\n",
    "        <br>\n",
    "        <h1 style=\"position: absolute;font-size: 300%;\">A primer on pandas</h1>\n",
    "    </div>\n",
    "    <div class=\"column\">\n",
    "        <img src=\"https://www.bjss.com/wp-content/uploads/BJSS.svg\"\n",
    "             alt=\"BJSS Logo\"\n",
    "             align=\"right\" \n",
    "             width = \"200\"\n",
    "             style=\"margin: 0px 60px\"\n",
    "             />\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#Background\"><font size=\"+1\">Background</font></a>\n",
    "* Learning Objectives\n",
    "\n",
    "<a href=\"#Recap-of-Day-1\"><font size=\"+1\">Recap of Day 1, Part 1</font></a>\n",
    "* Basic python data types\n",
    "* General python objects\n",
    "* Accessing the properties and methods of objects\n",
    "* Pandas essentials\n",
    "    * `import` pandas\n",
    "    * Read data\n",
    "    * Export data\n",
    "    * Select columns\n",
    "    * Filter rows\n",
    "    * Generate new variables\n",
    "\n",
    "<a href=\"#Getting-Started\"><font size=\"+1\">Getting Started</font></a>\n",
    "* Setting Your Working Directory\n",
    "* Importing pandas, reading data.\n",
    "* Reminder\n",
    "\n",
    "<a href=\"#Descriptive-Statistics\"><font size=\"+1\">Descriptive Statistics</font></a>\n",
    "* Describing numerical data\n",
    "* Descriptive statistics for numerical data\n",
    "* Describing text (or categorical) data\n",
    "* Sorting values\n",
    "* <a href=\"#Exercise-1\">Exercise 1</a>\n",
    "\n",
    "<a href=\"#Updating-Values\"><font size=\"+1\">Updating Values</font></a>\n",
    "* Copies and Views\n",
    "* Selecting and Filtering with `.loc[row_indexer, col_indexer]` and `.iloc[row_indices, col_indices]`\n",
    "* Updating DataFrame Cells\n",
    "* Propagating Missing Data Values with .loc[]\n",
    "* Changing column data types\n",
    "* Changing column names\n",
    "\n",
    "<a href=\"#Aggregation\"><font size=\"+1\">Aggregation</font></a>\n",
    "* Aggregation using the `.groupby()` method.\n",
    "\n",
    "<a href=\"#Crosstabs,-Contingency,-and-Two-Way-Tables\"><font size=\"+1\">Crosstabs, Contingency, and Two Way Tables</font></a>\n",
    "* Crosstabulation using the `pd.crosstab()` top-level function.\n",
    "* <a href=\"#Exercise-2\">Exercise 2</a>\n",
    "\n",
    "<a href=\"#Merging-Data\"><font size=\"+1\">Merging Data</font></a>\n",
    "* Indexical Data\n",
    "* The `.merge()` function\n",
    "* Types of Merge\n",
    "* <a href=\"#Exercise-3\">Exercise 3</a>\n",
    "\n",
    "<a href=\"#Consolidation\"><font size=\"+1\">Consolidation</font></a>\n",
    "* Today's Learning Objectives\n",
    "* Vignettes for functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This notebook should give you a working knowledge of pandas DataFrames (and Series), including how to:\n",
    "* Describe numeric and categorical data\n",
    "* Update values and columns\n",
    "* Aggregate and cross-tabulate data\n",
    "* Merge data\n",
    "\n",
    "and then we'll take an opportunity to move beyond pandas:\n",
    "* Exploring functions in python and their application in pandas\n",
    "\n",
    "You should read through this notebook, adding code for the exercises and testing your outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of Day 1, Part 1\n",
    "\n",
    "A brief recap of what we covered in day 1, part 1.\n",
    "\n",
    "## Data Types\n",
    "\n",
    "Numeric values - Integers (`int`; Whole numbers), Floating-point numbers (`float`; Decimals)  \n",
    "Text values - String (`str`; given in 'single quote' or \"speech marks\")  \n",
    "Logical values - Booleans (`bool`; `True` or `False` values)  \n",
    "\n",
    "## Objects\n",
    "\n",
    "Python is object-oriented, all general data types (numerics, strings, booleans) and data structures (lists, tuples, sets, dictionaries) are objects. Objects have properties and methods. Properties are data stored by the object, and methods are behaviours, procedures or functions that can be performed by the object to produce an output, often with respect to the properties of the object itself. Objects are extensible, this means that programmers can take very general data structures and produce specialised objects that are useful for particular purposes, like the Series and DataFrame objects that Pandas implements to make working with data easier.\n",
    "\n",
    "When we create an object we create a specific 'instance' of it, usually with some specific data. We store this new object in memory as a named variable. This name is called the object's identifier and allows us to reuse that object for as long as it remains in memory.\n",
    "\n",
    "When we name an object, the name can't start with numbers, contain special characters (e.g. ^, %, & etc.) or be a reserved word like `list` or `True` which mean something particular to python.\n",
    "\n",
    "## Accessing the properties and methods of objects\n",
    "\n",
    "Some special functions built-in to python allow you to directly access the properties and methods of objects, such as the `str()` function which gets a string representation of an object, or the `len()` which returns the length of an object.\n",
    "\n",
    "However, most of the properties and methods of a function are accessed using '.' (dot notation).\n",
    "\n",
    "For instance, getting the rows and columns of a pandas DataFrame:\n",
    "```python\n",
    "nrows, ncols = df.shape\n",
    "```\n",
    "In the code above, I have a DataFrame called `df`, I get the number of rows and columns in `df` using the `shape` property with `df.shape`. This property returns a tuple, which I 'unpack' and store in two variables `nrows` and `ncols`.\n",
    "\n",
    "We know that `shape` is a property of a DataFrame because it is just a word, there are no parentheses (brackets) after it. Compare it to:\n",
    "```python\n",
    "veg = ['carrot','carrot','parsnip','cabbage','carrot']\n",
    "veg.count('carrot')\n",
    "```\n",
    "In the above code, we make a list called `veg` which contains several string objects. We then call a list method on `veg` called `count()`, which counts the number of occurances of a given parameter in the list. The parameter we use is `'carrot'` which matches 3 items in `veg`, we thus expect the output 3 from the `count()` method.\n",
    "\n",
    "## Pandas Essentials\n",
    "\n",
    "### `import` the pandas module\n",
    "\n",
    "We've been using pandas, which is a powerful python module for data reading, writing and wrangling. By convention we import pandas into python using:\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "`pd` is a commonly using nickname or 'alias' for pandas.\n",
    "\n",
    "### Read Data Using pandas\n",
    "\n",
    "Pandas has a load of built in file readers, they are all prefixed with `.read`, you can look them up with the tab key and use the one you need.\n",
    "\n",
    "When you read data into python and assign it to a variable, we refer to that object as a 'DataFrame'.\n",
    "```python\n",
    "titanic = pd.read_csv('../Data/titanic.csv')\n",
    "```\n",
    "\n",
    "### Export Data Using pandas\n",
    "\n",
    "Pandas can export DataFrame or Series objects to a range of different data formats, they are all prefixed with `.to`.\n",
    "\n",
    "```python\n",
    "titanic.to_excel('../Save/titanic.xlsx')\n",
    "```\n",
    "\n",
    "### Select Columns\n",
    "\n",
    "* Select a single column by indexing with the column name.\n",
    "* Select multiple columns by indexing with a list of column names.\n",
    "\n",
    "```python\n",
    "titanic['survived'] # single column, returning a Series\n",
    "titanic[['survived','pclass']] # multiple columns, returning a DataFrame\n",
    "```\n",
    "\n",
    "### Filter Rows\n",
    "\n",
    "Rows are filtered by creating a 'Boolean mask', effectively a row-by-row list of `True` or `False` values that determine whether a row is included or excluded in the filtered dataframe.\n",
    "\n",
    "* Boolean masks are defined by setting a condition on the values in a given column.\n",
    "* Multiple conditions can be combined using 'and' or 'or' operators which are given by `&` and `|` respectively.\n",
    "\n",
    "Common conditions include:\n",
    "* `==` is 'equal to'.\n",
    "* `!=` is 'not equal to'.\n",
    "* `>` and `>=` are 'greater than', and 'greater than or equal to'.\n",
    "* `<` and `<=` are 'less than', and 'less than or equal to'.\n",
    "\n",
    "There are also some useful functions, including:\n",
    "* `.isin()` takes a list and return a `True` value for any row with a column value in that list.\n",
    "* `between()` takes an upper and lower value, and returns `True` for any row with a column value between those values.\n",
    "\n",
    "Finally, the 'tilde' `~` inverts, or negates whatever filter condition you have specified.\n",
    "\n",
    "```python\n",
    "titanic[titanic['survived'] == 0] # single condition filter\n",
    "titanic[(titanic['survived'] == 0) & (titanic['pclass'] > 1)] # multiple condition filter\n",
    "```\n",
    "\n",
    "### Chained Indexing\n",
    "We can combine column selection and row filtering in what is known as 'chained index'.\n",
    "```python\n",
    "titanic[(titanic['survived'] == 0) & (titanic['pclass'] > 1)]['survived'] # returns a Series\n",
    "```\n",
    "The order in which you chain operations doesn't matter.\n",
    "\n",
    "### Generate New Variables\n",
    "\n",
    "There are a number of ways to create new variables.\n",
    "\n",
    "New columns can be created directly based on arithmetic operations like + - / \\* \n",
    "\n",
    "```python\n",
    "titanic['family'] = titanic['sibsp'] + titanic['parch']  \n",
    "```\n",
    "Binary variable can be created with conditions\n",
    "```python\n",
    "titanic['child'] = (titanic['age'] < 18).astype(int)\n",
    "```        \n",
    "Continuous variables can be classified using `pd.cut()` or `pd.qcut()`\n",
    "```python\n",
    "titanic['fare_tertiles'] = pd.qcut(titanic['fare'], 3, ['low','mid','high'])\n",
    "```\n",
    "Also, variables can be removed using the `del` statement, or the `.drop()` method.\n",
    "```python\n",
    "del titanic['survived'] # delete a column\n",
    "titanic.drop(columns=['survived','embarked'], inplace = True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "## Setting Your Working Directory\n",
    "\n",
    "Remember from yesterday that you can check your working directory using `%pwd` and change it using `%cd`.\n",
    "\n",
    "In the code cell below check that your working directory is the 'Notebooks' folder within the Introduction to Python Folder, if it is not, change the directory to Notebooks.\n",
    "\n",
    "While it is not essential for you to set the working directory, not doing so will mean that you'll have to alter all the relative paths for data and solutions set up in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and set your working directory here.\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pandas and Read in Data\n",
    "\n",
    "Yesterday we learnt the code to import a python library. Use this to import pandas.\n",
    "\n",
    "We also learnt the code to read in some data into python using pandas. Read in the titanic csv dataset, and assign the DataFrame to a variable named `titanic`.\n",
    "\n",
    "Finally, check the first 5 rows of the DataFrame by calling the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here. Add additional cells if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder\n",
    "%load ../Solutions/Day2/import_and_read.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "There are really two broad types of data in our DataFrames at the moment that we want to look at - numerical data (i.e. ints and floats) and text data (i.e. strings; slightly confusingly called objects).\n",
    "\n",
    "In this section, we will explore some basic univariate descriptive statistics.\n",
    "\n",
    "## Describing Numerical Data\n",
    "Let's start with the numerical data, because thats the easiest to work with. Pandas even has a built in function called `.describe()` which will provide some descriptive statistics for all the numerical columns in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the titanic dataframe\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it happens, the descriptive statistics output for our titanic dataset is also a DataFrame!\n",
    "\n",
    "Make sure you understand what each row means in this table:\n",
    "* **count** - the number (count) of entries in the given column.\n",
    "* **mean** - the average (arithmetic mean) data value in the given column.\n",
    "* **std** - the standard deviation (spread) of values in the given column.\n",
    "* **min** - the smallest value in the given column.\n",
    "* **25%** - the value of the data at the lower quartile (i.e. after the first 25% of data, ordered from smallest to largest).\n",
    "* **50%** - the middle value of the data (aka the median), half the values are larger than this value, and half smaller.\n",
    "* **75%** - the value of the data at the upper quartile (i.e. after the first 75% of data, ordered from smallest to largest).\n",
    "* **max** - the maximum data value recorded.\n",
    "\n",
    "We can get a sense of the data from these descriptive statistics. For instance: \n",
    "\n",
    "## Descriptive Statistics for Numerical Data\n",
    "\n",
    "`.describe()` is great to get an overview, but what if we just wanted particular statistics and not the whole lot?\n",
    "\n",
    "Well, pandas will let you run a range of statistics individually! Some examples are given in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count() can be defined for all datatypes, so all columns are computed. Note which columns have some missing data.\n",
    "titanic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean() is only defined for numeric columns\n",
    "titanic[titanic['age'] < 18].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std() is also only defined for numeric columns\n",
    "titanic.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min() has a definition for numeric and text data.\n",
    "# The minimum value of a text field is the text which is first alphabetically.\n",
    "titanic.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max() has a definition for numeric and text data.\n",
    "# The maximum value of a text field is the text which is last alphabetically.\n",
    "titanic.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile() allows you to specify quantiles, such as 0.25 (lower quartile), 0.5 (median), and 0.75 (upper quartile)\n",
    "# for convenience median() also exists\n",
    "titanic['age'].quantile(0.5) # 25% - lower quartile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum() works to concatenate text, producing a curious output.\n",
    "titanic['fare'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hopefully though it is obvious that these methods could be called on selected columns too.\n",
    "titanic['fare'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it happens, python has a built-in sum, min and max functions which does the same thing.\n",
    "# however, pandas sum is better when confronted with missing data:\n",
    "sum(titanic['fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic[titanic['fare'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this instead\n",
    "sum(titanic[titanic['fare'].notnull()]['fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, a new filter condition for working with missing data is apparent: `notnull()` this returns `True` for rows that have a valid value, and `False` otherwise. Similar to the behaviour of `bool()`. The opposite of `notnull()` is `isnull()`.\n",
    "\n",
    "This method of selection is similar to making conditional statements with object methods that return a Boolean, e.g.\n",
    "```python\n",
    "if string_variable.islower():\n",
    "    # Do something\n",
    "```\n",
    "The same principle can apply to other contexts, for instance the `Series` object has a large number of string methods collected as `.str.`, calling `titanic['name'].str.contains('Mr.', regex=False)` returns `True` or `False` for each row in a column depending on whether it contains the substring 'Mr.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select passengers with title Mr. and get mean fare\n",
    "titanic[titanic['name'].str.contains('Mr.', regex=False)]['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select passengers with title Mrs. and get mean fare\n",
    "titanic[titanic['name'].str.contains('Mrs.', regex=False)]['fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing Text (or Categorical) Data\n",
    "\n",
    "We can still use `describe()` to look at text data, however we need to specify that we're looking at object (text) data types.\n",
    "\n",
    "Really, the descriptive statistics below are for categorical data, they don't work very well if every value in a field is a different piece of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the describe parameters we're only choosing to include object datatypes, given by 'O'.\n",
    "# The 'O' is in a list, because we could include other data types in the list if we wanted to.\n",
    "titanic.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are describing an object you get some different summary statistics than with numerical data:\n",
    "\n",
    "* **count** as before, a count of the values present in each column.\n",
    "* **unique** a count of the number of unique values in each column.\n",
    "* **top** is the most common value - aka the mode.\n",
    "* **freq** is the frequency of occurance of the most common value.\n",
    "\n",
    "Let's dig a bit deeper into some of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interestingly there are 2 Kate Connollys, however they don't appear to be duplicates.\n",
    "titanic[titanic['name'] == 'Connolly, Miss. Kate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way we could check for other name duplicates is by taking the mode.\n",
    "# As mode can be non-unique it returns a series\n",
    "# Looks like James Kelly is another possible duplicate.\n",
    "titanic['name'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, these appear to be different people!\n",
    "titanic[titanic['name'] == 'Kelly, Mr. James']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a clever way of looking at the modal values!\n",
    "# The Series returned by mode is an iterable, so is a valid parameter for .isin()\n",
    "titanic[titanic['name'].isin(titanic['name'].mode())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unique() function will give us all the unique objects in a column.\n",
    "titanic['embarked'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value_counts() function gives a count for each unique value in a chosen column.\n",
    "titanic['embarked'].value_counts(dropna=False)\n",
    "# Most people embarked in Southampton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Data\n",
    "\n",
    "Sorting data is straightforward in pandas, a simple sort on one columns used the DataFrame method `.sort_values()`:\n",
    "```python\n",
    "titanic.sort_values('age')\n",
    "```\n",
    "The default is to sort in ascending order, from smallest to largest value. Set the ascending parameter to `False` for a descending sort:\n",
    "```python\n",
    "titanic.sort_values('age', ascending = False)\n",
    "```\n",
    "This approach sorts and returns the entire DataFrame, if you want to sort a single column on its works similarly:\n",
    "```python\n",
    "titanic['age'].sort_values()\n",
    "```\n",
    "More complicated sorting behaviours can be managed by passing a list, in the order you would like the sort to occur:\n",
    "```python\n",
    "titanic.sort_values(['pclass','age'], ascending = [True, False])\n",
    "```\n",
    "In the above code I sort first by 'pclass' then by 'age'. in addition I pass a list to ascending indicating that 'pclass' is to be sorted in ascending order, and 'age' in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort fare descending\n",
    "titanic.sort_values('fare', ascending = False).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by sex ascending, then age descending\n",
    "titanic.sort_values(['sex','age'], ascending = [True, False]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by sex descending, then age descending\n",
    "titanic.sort_values(['sex','age'], ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "1. How old is the oldest passenger in the dataset?\n",
    "2. How many men and women are in the dataset?\n",
    "    * Check the pd.Series.value_counts() docstring and figure out how to get proportions of men and women.\n",
    "3. Create a new column called 'std_fare' which is the 'fare' minus the mean fare, divided by the standard deviation.\n",
    "4. Calculate the number of children in second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 solutions\n",
    "%load ../Solutions/Day2/exercise1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Values\n",
    "\n",
    "\n",
    "## Important! Copies and Views\n",
    "\n",
    "The selecting of columns and filtering of rows that you've done above effectively **copies** the original dataframe to make a new one based on your conditions. This is great for creating a pared-down dataframe to work with, or when filtering data to produce statistics for particular subsets of data.\n",
    "\n",
    "However, when we want to actually update cell values we need to use a special function that ensures we are editing the original dataframe **view** and not a **copy** of it. If we edit cells on a copy of a dataframe, we might find that these values are not updated in the original dataframe when we come to analyse it!\n",
    "\n",
    "This is a technical point that you don't need to worry too much about, just remember to use the following approaches when dealing with code!\n",
    "\n",
    "In the titanic data, given what we know so far, we might try to update a cell values using what's called 'chained indexing':\n",
    "\n",
    "```python\n",
    "titanic[titanic['embarked'] == 'C']['embarked'] = 'c'\n",
    "```\n",
    "The above code makes sense - filter the rows, select the column and assign the value you want. However, python will give you a warning (specifically a 'SettingWithCopyWarning') that you're not doing things properly!\n",
    "\n",
    "Instead we'll do the following:\n",
    "\n",
    "```python\n",
    "titanic.loc[titanic['embarked'] == 'C','embarked'] = 'c'\n",
    "```\n",
    "\n",
    "The code looks very similar, however in the second (correct) example we're using `.loc[]`\n",
    "\n",
    "`.loc[]` is actually almost identical to the selecting and filtering we've already done. We just specify the rows and columns within the square bracket like: `.loc[row_indexer, col_indexer]` where:\n",
    "* `row_indexer` is the mask if we are filtering, or a colon, `:`, if we want to include all rows.\n",
    "* `col_indexer` is a single column name, or a list of column names, or a colon, `:`, if we want to include all columns.\n",
    "\n",
    "You can actually use the .loc[] approach to do all the selecting and filtering we've already done if you want.\n",
    "\n",
    "In addition to `.loc[]` we also have `.iloc[]` which behaves very similarly, except that it allows us to index on the index position of rows and columns.\n",
    "\n",
    "Some examples using iloc[]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows, and the first column\n",
    "titanic.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or select a range of columns\n",
    "titanic.iloc[:,3:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another head-like indexing approach\n",
    "titanic.iloc[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some examples using `.loc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often researchers use column selection to reorder the columns in a DataFrame.\n",
    "cols = list(titanic.columns)\n",
    "cols.reverse()\n",
    "titanic.loc[:,cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc takes row conditions as usual.\n",
    "titanic.loc[titanic['pclass'].isin([1,2]),['pclass','name']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often pandas users will use chained indexing when selecting and filtering, and the `loc` and `iloc` operators when cleaning values on a cell-by-cell basis. There is no reason why you can't use these operators for selecting and filtering too, however, if you want to assign the sub-set DataFrame to a new variable you'll need to use the `.copy()` method to ensure that it is a different DataFrame in memory, rather than a reference to the original.\n",
    "```python\n",
    "new_df = titanic.loc[:,['survived','embarked']].copy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating Missing Data Values with `.loc[]`\n",
    "\n",
    "Earlier, when we created a new binary variable called 'child' the condition we used created a `Series` of `True` and `False` values based on whether the condition was met or not.\n",
    "\n",
    "An important consideration when creating a new variable is presence of missing data. A condition will automatically set a missing value to `False`. This is fine for filtering data, but misrepresents data in a new variable - it makes a derived variable appear more complete than it is in reality.\n",
    "\n",
    "In order to update the 'child' column to incorporate rows we know are missing we can use `.loc[]`:\n",
    "```python\n",
    "# This was the column originally generated.\n",
    "titanic['child'] = (titanic['age'] < 18).astype(int)\n",
    "# Now, update the missing data values\n",
    "titanic.loc[titanic['age'].isnull(),'child'] = titanic[titanic['age'].isnull()]['age']\n",
    "```\n",
    "In the 2nd line of code, we use the row filter `titanic['age'].isnull()` to filter missing data in the 'age' column and we select the 'child' column. These are used to create a 'view' of the titanic DataFrame using `.loc[]`.\n",
    "\n",
    "Then, we assign to that combination of rows and columns the values given by: `titanic[titanic['age'].isnull()]['age']` which will be a Series of missing values. We should now see missing values in the 'child' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create child columns\n",
    "titanic['child'] = (titanic['age'] < 18).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial composition of 'child'\n",
    "titanic['child'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update and check new field\n",
    "titanic.loc[titanic['age'].isnull(),'child'] = titanic[titanic['age'].isnull()]['age']\n",
    "titanic['child'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Column Data Types\n",
    "\n",
    "As with basic python data types, we can cast columns of data from one data type to another. This can be useful as part of a data cleaning process. Sometimes we find that data we expect to be numeric is actually string data, often occurs because the numbers are actually stored as text in the original dataset, in which case they have to be converted. Other times it is because the original dataset includes characters that pandas won't immediately interpret as a numeric value. This is particularly the case if a dataset contains missing data that is coded to a special character. Reading in these data as text is the safest option, as it preserves all of the information and requires the data analyst to make a decision as to how to handle the conversion to a numeric data type.\n",
    "\n",
    "There are two ways to change a datatype, firstly the Series method: `.astype()` in which the parameter is a data type e.g.\n",
    "```python\n",
    "titanic['survived'].astype(str)\n",
    "```\n",
    "There is also a 'top-level' pandas function `pd.to_numeric()` which is a good option for data cleaning.\n",
    "```python\n",
    "pd.to_numeric(titanic['survived'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn survived to a string column\n",
    "titanic['survived'] = titanic['survived'].astype(str)\n",
    "titanic.dtypes['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turned survived back to a numeric\n",
    "titanic['survived'] = pd.to_numeric(titanic['survived'])\n",
    "titanic.dtypes['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Column Names\n",
    "\n",
    "We've been dealing with clean data so far - but what happens when you're using data that's intended for something else? Data formatted for the web isn't ideal for using in programming.\n",
    "\n",
    "What are some issues can we have in column names?\n",
    "\n",
    "* column names might be really long, thesea are a pain to type out.\n",
    "* spaces between words, and presence of special characters (e.g. %^&£ etc.) can be annoying\n",
    "* columns names may be ambiguous or not sufficiently descriptive.\n",
    "\n",
    "We can use the pandas `.rename()` method to update column names. To update the column names we pass a dicitonary to the parameter 'columns', e.g.\n",
    "```python\n",
    "df.rename(columns={'two':'new_name'}, inplace=True)\n",
    "```\n",
    "For a DataFrame `df`, the `rename` method allows us to change the names of columns based upon a dictionary. Here, the dictionary key `'two'` is the current name of the column, and the dictionary value `'new_name'` is what we want to rename the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.rename(columns={'std_fare':'fare_zscore'}, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "\n",
    "Aggregation means grouping data together by a particular grouping variable and producing a summary of one or more columns for that grouping variable.\n",
    "\n",
    "We'll use the `groupby()` function. \n",
    "\n",
    "This function can be really useful, especially when your data are disaggregate - e.g. data about individual units of people or things. \n",
    "\n",
    "`groupby()` allows you to aggregate by a categorical variable and summarise numerical data into a new dataframe.\n",
    "\n",
    "`.groupby()` works on a principle known as 'split-apply-combine':\n",
    "* Split - a dataframe is divided into a set of smaller dataframes based on the grouping variable.\n",
    "* Apply - an aggregation is applied to each of the groups to create a single row for each group in the original dataframe.\n",
    "* Combine - bring together the aggregated dataframe rows into a final new dataframe.\n",
    "\n",
    "Let's walk through what that might look like for the `titanic` dataframe:\n",
    "* Firstly, we decide to **split** the data by the 'pclass'. This divides the `titanic` dataframe into effectively three separate dataframes, one for first, one for second and one for third class.\n",
    "* Secondly, we **apply** an aggregation to the dataframe. You can either produce an aggregate statistic for all rows, or you can selected specific columns on which to do the aggregation. If we **apply** a `.mean()` aggregation to 'fare', then for each 'pclass' group we get the average fare cost.\n",
    "* Finally, pandas returns a **combined** dataframe that contains the new aggregate statistics.\n",
    "\n",
    "Let's look at that in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('pclass')['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly a count of children by class might look like this:\n",
    "titanic.groupby('pclass')['child'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this all sounds fairly straightforward! `.groupby()` is a powerful tool, particularly when you are working with any kind of hierarchical data where you might want to know something aggregate about the groups within the data, for instance:\n",
    "* individuals nested in households.\n",
    "* employees nested in firms.\n",
    "* patients nested in primary or secondary care trusts.\n",
    "* small area geographies (e.g. wards, output areas, postcodes etc.) nested in larger geographies (e.g. districts, counties etc.)\n",
    "* countries nested in supra-national entities.\n",
    "\n",
    "or, demographic, cultural and socio-economic classes:\n",
    "* individuals by age, sex, ethnicity, religion etc.\n",
    "* employees by grade or occupational social class.\n",
    "* households by neighbourhood deprivation rank or decile.\n",
    "* experimental subjects in intervention and control arms of a trial.\n",
    "\n",
    "We can also aggregate according to more complicated groupings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby passenger class, then city of embarkation.\n",
    "titanic.groupby(['pclass','embarked'])['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB order is important to the output.\n",
    "titanic.groupby(['embarked','pclass'])['fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ordering of groups may be important as it affects the resultant DataFrame.\n",
    "\n",
    "If you assign the `groupby()` output to a variable, you can also pull out dataframes for particular groups, just as if you had written a filter statement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = titanic.groupby('pclass')\n",
    "classes.get_group(3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosstabs, Contingency, and Two-Way Tables\n",
    "\n",
    "Similar to aggregation with `.groupby()` a cross-tabulation table allows you to generate frequencies for combinations of groups of data.\n",
    "\n",
    "Crosstabs are often also refered to as 'contingency tables' and 'two-way tables' and, although there may be some subtle distinctions, these all refer to the setting of one categorical variable against another, creating a matrix, and then counting, or otherwise aggregating by cell values.\n",
    "\n",
    "To create cross-tabulations, you can use the `pandas.crosstab()` function.\n",
    "\n",
    "Let's have a look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab of survived against sex\n",
    "pd.crosstab(titanic['survived'],titanic['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic use of `.crosstab()` requires that you input the row category ('survived') and the column category ('sex'). This produces cell counts for the cross-tabulation of the two categories, so there were 339 passengers who were both 'female' and 'survived'.\n",
    "\n",
    "The basic crosstab can be augmented with some additional parameters:\n",
    "* `margins` - set to `True` to add row and column totals.\n",
    "* `normalize` - create row or column proportions, or overall proportions, rather than frequencies. Keywords are 'index', 'columns' or 'all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab of survived against sex\n",
    "pd.crosstab(titanic['survived'],titanic['sex'], margins=True, normalize='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "1. What is the average fare paid by men and women?\n",
    "2. What is the median fare paid by men and women in each different class?\n",
    "3. Create a crosstab for 'survived' and 'pclass', which class is the best for survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 solutions\n",
    "%load ../Solutions/Day2/exercise2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Data\n",
    "\n",
    "Often all the data you need to answer a question are not contained within a single dataset, but across several. Datasets can be joined, or 'merged', to allow data to be analysed together, but only **if the two datasets share a common reference or identifier.**\n",
    "\n",
    "Linking data come in a number of forms, and are commonly refered to as 'indexical' data. Some examples include:\n",
    "\n",
    "* Your NHS number, allowing data linkage across the NHS for primary, secondary, tertiary care episodes and prescribing.\n",
    "* Any account number (e.g. banking, utilities, travel card, council tax etc.) can acts as a point of linkage between different sets of data.\n",
    "* Your email, phone number, social media handles etc.\n",
    "* Your address can also act as a spatial reference, linking you to your neighbourhood, local services etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to merge\n",
    "\n",
    "While you may have heard it called \"Join\" in other languages, particularly in database query languages, in pandas we use the `.merge()` function.\n",
    "\n",
    "Once you have established that two DataFrames share a reference that will permit a merge to be conducted, you may wish to further specify how the merge behaves with the `how` parameter.\n",
    "\n",
    "* Inner - Only rows with reference values that appear in both DataFrames are merged.\n",
    "* Left - All the data from the 'left' DataFrame is retained, and any rows that have matching references are merged from the 'right' DataFrame.\n",
    "* Right - All data from the right and anything that matches from the left. Effectively, the reverse of 'Left'.\n",
    "* Outer (Full) - all data from the left and right DataFrame is retained, matched up where possible.\n",
    "\n",
    "This can be easier to understand graphically:\n",
    "\n",
    "![joins](https://www.dofactory.com/img/sql/sql-joins.png)\n",
    "\n",
    "Let’s read in some additional titanic data and have a look at them.\n",
    "\n",
    "The new dataset includes the passenger name and age, as well as the additional variables:\n",
    "* boat - lifeboat identifier\n",
    "* body - body identification number\n",
    "* home.dest - the passenger's home and destination in the form \"home / dest\" or just \"home\".\n",
    "\n",
    "The dataset is located in the 'Data' folder, it is an excel file called: 'titanic_more.xlsx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the titanic_more.xlsx using pandas.\n",
    "titanic_more = pd.read_excel('../Data/titanic_more.xlsx')\n",
    "titanic_more.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to merge the two tables we need to use a column which uniquely defines each passenger and is available in both DataFrames. At first glance, 'name' would appear to be a good candidate for this, however, remember there are a couple of passengers who have the same name as each other.\n",
    "\n",
    "We can explicitly check is a column uniquely identifies rows with the `Series.is_unique` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name is not unique defined for each row in titanic\n",
    "titanic['name'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name is not unique defined for each row in titanic_more\n",
    "titanic_more['name'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can create a field that will uniquely identify passengers in both datesets by generating a new variable that combines the 'name' and 'age' variables. This is because we happen to know the ages of the passengers who have the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique id base don name and age for titanic\n",
    "titanic['name_age_id'] = titanic['name'] + \" \" + titanic['age'].astype(str)\n",
    "# Check if the new variable is unique\n",
    "titanic['name_age_id'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique id base don name and age for titanic\n",
    "titanic_more['name_age_id'] = titanic_more['name'] + \" \" + titanic_more['age'].astype(str)\n",
    "# Check if the new variable is unique\n",
    "titanic_more['name_age_id'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have unique id fields in both titanic and titanic_more, we use them to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the titanic_more dataset with titanic\n",
    "titanic_merge = titanic.merge(titanic_more[['name_age_id','boat','body','home.dest']], on = 'name_age_id')\n",
    "titanic_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above we merge the `titanic_more` DataFrame into `titanic`. We do this on the basis of the 'name_age_id' variable that we created.\n",
    "\n",
    "The default merge behaviour is 'inner' join, however in this particular case all behaviours ('inner', 'left', 'right','outer') resolve to the same outcome as both datasets include the same 1,309 passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that pandas actually handles multiple columns directly for unique identification.\n",
    "titanic_merge = titanic.merge(titanic_more[['name','age','boat','body','home.dest']], on = ['name','age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Load the revised dataset 'titanic_revised.xlsx' which only includes passengers which have a value for boat, body or home.dest and try merging this to titanic.\n",
    "\n",
    "1. Which types of merge perform as expected?\n",
    "    * Try the different `how` parameters: 'inner', 'outer', 'left', 'right'.\n",
    "2. How many values are observed for boat, body, and home.dest?\n",
    "    * i.e. How many values are non-missing/\n",
    "    * Hint: use `.count()`\n",
    "3. How many passengers in the dataset used lifeboat number 3? What proportion of them were female?\n",
    "    * Hint: think about datatypes.\n",
    "4. How many passengers record 'New York' or 'NY' somewhere in the 'home.dest' column?\n",
    "    * If you do a selection using `Series.str.contains()` you need to specify the parameter na=False.\n",
    "    * This sets missing values to `False` in the boolean filter and excludes them from the selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions for exercise 3\n",
    "%load ../Solutions/Day2/exercise3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidation\n",
    "\n",
    "## Reminder of Learning Objectives\n",
    "\n",
    "We're going to establish a working knowledge of pandas DataFrames (and Series), including how to:\n",
    "* Describe numeric and categorical data\n",
    "* Update values and columns\n",
    "* Aggregate and cross-tabulate data\n",
    "* Merge data\n",
    "\n",
    "## Vignettes\n",
    "\n",
    "There is a supplementary additional notebook to explore in the notebooks folder.\n",
    "1. [Vignette - Functions](Vignette-User_Defined_Functions.ipynb): explores how to create functions in python using `def` and `lambda` and how to `map` and `apply` functions to pandas dataframes.\n",
    "\n",
    "## Looking ahead: Tomorrow's challenge\n",
    "The notebooks you've covered today will hopefully set you up well for the data cleaning task tomorrow, set out in the [Playing-with-Lego](playing-with-lego.ipynb) notebook. A lot of the kind of methods you'll want to use there will have been covered in these notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
